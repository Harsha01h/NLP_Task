# -*- coding: utf-8 -*-
"""NLP  Tasks.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tePxMtfvStRqmn73ByafeAxItk_cYxD9
"""

#task1

import re
pattern1 = re.compile(r'abc')
result1 = pattern1.match('abcdef')
if result1:
  print("Match found:", result1.group())
pattern2 = re.compile(r'.')
result2 = pattern2.search('Hello')
if result2:
  print("Character found:", result2.group())
pattern3 = re.compile(r'[aeiou]')
result3 = pattern3.search('Hello')
if result3:
  print("Vowel found:", result3.group())
pattern4 = re.compile(r'\d{3}-\d{2}-\d{4}')
result4 = pattern4.match('123-45-6789')
if result4:
  print("Social Security Number:", result4.group())
pattern5 = re.compile(r'\.')
result5 = pattern5.search('www.example.com')
if result5:
  print("Dot found:", result5.group())
pattern6 = re.compile(r'(\d{2})/(\d{2})/(\d{4})')
result6 = pattern6.match('01/09/2024')
if result6:
  print("Day:", result6.group(1))
  print("Month:", result6.group(2))
  print("Year:", result6.group(3))
pattern7 = re.compile(r'cat|dog')
result7 = pattern7.search('I have a cat and a dog')
if result7:
  print("Animal found:", result7.group())

#Task 2
import nltk
from nltk.tokenize import word_tokenize
nltk.download('punkt')
nltk.download('punkt_tab')
text = "Tokenization without transformers is straightforward with tools like NLTK."
tokens = word_tokenize(text)
print("Tokens:", tokens)
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
text = "Tokenization without transformers is straightforward with tools like NLTK."
tokens_transformers = tokenizer(text, return_tensors="pt")
print("Transformers Tokens:", tokens_transformers)
tokens_transformers_list = tokenizer.convert_ids_to_tokens(tokens_transformers['input_ids'][0].numpy().tolist())
print("Transformers Tokens (List):", tokens_transformers_list)
decoded_text = tokenizer.decode(tokens_transformers['input_ids'][0], skip_special_tokens=True)
print("Decoded Text:", decoded_text)

#task 3
import numpy as np
import nltk
from nltk.tokenize import word_tokenize
import spacy
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

nltk.download('punkt')
nltk.download('punkt_tab')
spacy.cli.download("en_core_web_sm")
nlp = spacy.load("en_core_web_sm")

corpus = """One disadvantage of using 'Best Of' samping is that it may lead to limited exploration of the model's
knowledge and creativity. By focusing on the most probable next words, the model might generate responses that are
safe and conventional, potentially missing out on more diverse and innovative outputs. The lack of exploration could
result in repetitive or less imaginative responses, especially in situations where novel and unconventional ideas are
desired.To address this limitation, other sampling strategies like temperature-based sampling or top-p (nucleus) sampling
can be employed to introduce more randomness and encourage the model to explore a broader range of possibilities.
However, it's essential to carefully balance exploration and exploitation based on the specific requirements of the task or
application."""

tokens = word_tokenize(corpus)
lemmatized_tokens = [token.lemma_ for token in nlp(corpus)]

all_tokens = tokens + lemmatized_tokens

tokenizer = Tokenizer()
tokenizer.fit_on_texts(all_tokens)
total_words = len(tokenizer.word_index) + 1

input_sequences = []
for token in all_tokens:
    token_list = tokenizer.texts_to_sequences([token])[0]

    for i in range(1, len(token_list)):
        n_gram_sequence = token_list[:i+1]
        input_sequences.append(n_gram_sequence)

input_sequences = []
for i in range(1, len(all_tokens)):
    token_list = tokenizer.texts_to_sequences(all_tokens[:i+1])
    token_list = [item for sublist in token_list for item in sublist]
    input_sequences.append(token_list)

max_sequence_length = max(len(seq) for seq in input_sequences)
input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_length, padding='pre')

X, y = input_sequences[:, :-1], input_sequences[:, -1]
y = np.array(y)


model = Sequential()
model.add(Embedding(total_words, 100, input_length=max_sequence_length-1))
model.add(LSTM(100))
model.add(Dense(total_words, activation='softmax'))

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

model.fit(X, y, epochs=5, verbose=1)

#task 4
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.probability import FreqDist
nltk.download('punkt')
nltk.download('stopwords')
def tokenize_document(document):
    tokens = word_tokenize(document)
    return [word.lower() for word in tokens if word.isalpha()]
def remove_stopwords(tokens):
    stop_words = set(stopwords.words('english'))
    return [word for word in tokens if word not in stop_words]
def find_morphology(tokens):
    fdist = FreqDist(tokens)
    return fdist.most_common(10)
document_text =""" Technology has changed the way we live, work, and communicate.
From smartphones to artificial intelligence,it continues to shape our daily lives.
In education, online learning platforms have made knowledge more accessible than ever.
Healthcare has also benefited, with advanced tools improving diagnosis and treatment.
Businesses use technology to streamline operations and reach global markets.
Social media connects people across continents, though it also raises concerns about privacy and mental health.
Despite its many advantages, technology can widen the gap between those with access and those without.
Environmental impacts, such as e-waste and energy use, must also be addressed.
As society grows more reliant on technology, ethical and responsible development becomes crucial.
Balancing progress with sustainability is one of the biggest challenges of our time.
"""
tokens = tokenize_document(document_text)
tokens_without_stopwords = remove_stopwords(tokens)
morphology = find_morphology(tokens_without_stopwords)
print("Morphology of the document:")
for word, frequency in morphology:
    print(f"{word}: {frequency}")

#task 5

import string
import random
import nltk
from nltk.corpus import stopwords, reuters
from collections import Counter, defaultdict
from nltk import FreqDist, ngrams
nltk.download('punkt')
nltk.download('punkt_tab')
nltk.download('stopwords')
nltk.download('reuters')
sents = reuters.sents()
stop_word = set(stopwords.words('english'))
extra_punct = set(['"', '-', '_'])
removal_list = set(stop_word) | set(string.punctuation) | extra_punct | set(['\t', 'rt'])
unigram = []
bigram = []
trigram = []
for sentence in sents:
    sentence = [word.lower() for word in sentence]
    sentence = [word for word in sentence if word != '.']
    unigram.extend(sentence)
    bigram.extend(list(ngrams(sentence, 2, pad_left=True, pad_right=True)))
    trigram.extend(list(ngrams(sentence, 3, pad_left=True, pad_right=True)))
def remove_stopwords(ngram_list):
    filtered = []
    for ngram in ngram_list:
        if any(word is None for word in ngram):
            filtered.append(ngram)
        else:
            if not any(word in removal_list for word in ngram):
                filtered.append(ngram)
    return filtered
unigram = [word for word in unigram if word not in removal_list]
bigram = remove_stopwords(bigram)
trigram = remove_stopwords(trigram)
freq_uni = FreqDist(unigram)
freq_bi = FreqDist(bigram)
freq_tri = FreqDist(trigram)
d = defaultdict(Counter)
for (a, b, c), freq in freq_tri.items():
    if a is not None and b is not None and c is not None:
        d[(a, b)][c] += freq
def pick_word(counter):
    "The sun was shining brightly over the hills.Birds chirped happily as the morning breeze blew gently."
    words, weights = zip(*counter.items())
    return random.choices(words, weights=weights)[0]
prefix = ("he", "is")
print(" ".join(prefix))
s = " ".join(prefix)
for i in range(19):
    counter = d[prefix]
    if not counter:
        prefix = random.choice(list(d.keys()))
        counter = d[prefix]
    suffix = pick_word(counter)
    s += ' ' + suffix
    prefix = (prefix[1], suffix)
print(s)

#Task 6
from nltk.util import ngrams
from nltk.lm import Laplace
from nltk.tokenize import word_tokenize
from nltk.lm.preprocessing import padded_everygram_pipeline
import nltk
nltk.download('punkt')
nltk.download('punkt_tab')
def ngram_smoothing(sentence, n):
  tokens = word_tokenize(sentence.lower())
  train_data, padded_sents = padded_everygram_pipeline(n, tokens)
  model = Laplace(n)
  model.fit(train_data, padded_sents)
  return model

sentence = input("Enter a sentence: ")
n = int(input("Enter the value of N for N-grams: "))
model = ngram_smoothing(sentence, n)
context = tuple(sentence.lower().split()[-n+1:])
next_words = model.generate(3, text_seed=context)
print("Next words:", ' '.join(next_words))

# Task 6
from nltk.util import ngrams
from nltk.lm import Laplace
from nltk.tokenize import word_tokenize
from nltk.lm.preprocessing import padded_everygram_pipeline
import nltk

# Download required tokenizer models
nltk.download('punkt')
nltk.download('punkt_tab')

def ngram_smoothing(sentence, n):
    tokens = word_tokenize(sentence.lower())
    # Wrap tokens in a list to simulate a corpus with one sentence
    train_data, padded_sents = padded_everygram_pipeline(n, [tokens])
    model = Laplace(n)
    model.fit(train_data, padded_sents)
    return model

sentence = input("Enter a sentence: ")
n = int(input("Enter the value of N for N-grams: "))

model = ngram_smoothing(sentence, n)

# Extract context: last n-1 tokens from the sentence
context = tuple(sentence.lower().split()[-(n-1):])

# Generate next 3 words based on context
next_words = model.generate(3, text_seed=context)

print("Next words:", ' '.join(next_words))

#Task 7
import nltk
nltk.download('all')
from nltk.tag import HiddenMarkovModelTagger
from nltk.corpus import treebank
nltk.download('treebank')
corpus = treebank.tagged_sents()
train_data = corpus[:3000]
test_data = corpus[3000:]
hmm_tagger = HiddenMarkovModelTagger.train(train_data)
sentence = input()
tokens = nltk.word_tokenize(sentence)
tagged_sentence = hmm_tagger.tag(tokens)
print(tagged_sentence)

#Task 8
import nltk
from nltk.corpus import treebank
from nltk.tag import hmm
from nltk.classify import MaxentClassifier
!pip install -U nltk
nltk.download('maxent_treebank_pos_tagger')
from nltk.tag import PerceptronTagger, StanfordTagger
nltk.download('treebank')
nltk.download('maxent_ne_chunker')
nltk.download('words')
nltk.download('averaged_perceptron_tagger')
corpus = list(treebank.tagged_sents())
train_data = corpus[:int(0.8 * len(corpus))]
test_data = corpus[int(0.8 * len(corpus)):]
hmm_tagger = hmm.HiddenMarkovModelTrainer().train(train_data)
hmm_accuracy = hmm_tagger.evaluate(test_data)
print(f"HMM Tagger Accuracy: {hmm_accuracy:.4f}")

import nltk
from sklearn.metrics import accuracy_score
import numpy as np
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')

def hmm_pos_tagger(sentence):
    tokens = nltk.word_tokenize(sentence)
    tagged = nltk.pos_tag(tokens)
    return tagged
def log_linear_pos_tagger(sentence):
    tokens = nltk.word_tokenize(sentence)
    tagged = nltk.pos_tag(tokens)
    return tagged

def compare_performance(sentence):
    hmm_tags = hmm_pos_tagger(sentence)
    log_linear_tags = log_linear_pos_tagger(sentence)
    gold_standard_tags = [tag for _, tag in hmm_tags]
    hmm_predicted_tags = [tag for _, tag in hmm_tags]
    log_linear_predicted_tags = [tag for _, tag in log_linear_tags]
    hmm_accuracy = accuracy_score(gold_standard_tags, hmm_predicted_tags)
    log_linear_accuracy = accuracy_score(gold_standard_tags, log_linear_predicted_tags)

    print("HMM Predicted Tags:", hmm_predicted_tags)
    print("Log-Linear Model Predicted Tags:", log_linear_predicted_tags)
    print("\n--- Performance Metrics ---")
    print("HMM Accuracy:", hmm_accuracy)
    print("Log-Linear Model Accuracy:", log_linear_accuracy)
input_text = "The quick brown fox jumps over the lazy dog."
compare_performance(input_text)

